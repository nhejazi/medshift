utils::globalVariables(
  c("..eif_component_names", "..w_names", "A", "L", "v_est", "s_est",
    "u_est", "u_diff_est")
)

#' Cross-validated Efficient Influence Function for Stochastic Effects
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medshift}}.
#' @param delta A \code{numeric} value indicating the magnitude of shift in the
#'  treatment that defines the counterfactual quantity of interest. In the case
#'  of binary treatments, this defines the incremental propensity score shift,
#'  which acts as a multiplier of the odds with which an observational unit
#'  receives treatment (EH Kennedy, 2018; <doi:10.1080/01621459.2017.1422737>).
#'  When the treatment is quantitative, the defines the degree of shift for a
#'  modified treatment policy but non-binary treatments are not  yet supported.
#' @param g_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model.
#' @param e_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a reparametrized model
#'  for the mediator(s) that is equivalent to a propensity score conditioning
#'  on the mediators.
#' @param m_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit an outcome regression.
#' @param phi_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a nuisance regression of
#'  a "derived" pseudo-outcome (contrast of outcome regressions under different
#'  treatment conditions), conditional on baseline covariates. This is of the
#'  following form phi(W) = E[m(A = 1, Z, W) - m(A = 0, Z, W) | W).
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by a call to the wrapper function \code{medshift}.
#' @param z_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (Z). The input for this argument is automatically
#'  generated by a call to the wrapper function \code{medshift}.
#'
#' @importFrom data.table data.table
#' @importFrom origami training validation fold_index
#'
#' @keywords internal
stoch_cv_eif <- function(fold,
                         data,
                         delta,
                         g_learners,
                         e_learners,
                         m_learners,
                         phi_learners,
                         w_names,
                         z_names) {
  # make training and validation data
  train_data <- origami::training(data)
  valid_data <- origami::validation(data)

  # compute nuisance parameters eta = (g, m, e, phi)
  ## 1) fit regression for incremental propensity score intervention
  g_out <- lapply(delta, function(delta) {
    # NOTE: even this is _repeated_ computation since delta is a multiplier
    #       ...worth fixing later if a bottleneck.
    g_est_delta <- fit_g_mech(
      data = train_data, valid_data = valid_data,
      delta = delta,
      learners = g_learners, w_names = w_names
    )
    return(g_est_delta)
  })

  ## 2) fit clever regression for treatment, conditional on mediators
  e_out <- fit_e_mech(
    data = train_data, valid_data = valid_data,
    learners = e_learners,
    z_names = z_names, w_names = w_names
  )

  ## 3) fit regression for incremental propensity score intervention
  m_out <- fit_m_mech(
    data = train_data, valid_data = valid_data,
    learners = m_learners,
    z_names = z_names, w_names = w_names
  )

  ## 4) difference-reduced dimension regression with pseudo-outcome
  phi_est <- fit_phi_mech(
    train_data = train_data, valid_data = valid_data,
    learners = phi_learners,
    m_out = m_out, w_names = w_names
  )

  # loop over each delta-shift in grid to assemble EIF components
  eif_over_delta <- lapply(seq_along(delta), function(delta_iter) {
    # compute component Dzw from nuisance parameters
    Dzw_groupwise <- compute_Dzw(
      g_out = g_out[[delta_iter]], m_out = m_out
    )
    D_ZW <- Dzw_groupwise$dzw_cntrl + Dzw_groupwise$dzw_treat

    # compute component Da from nuisance parameters
    g_pred_A1 <- g_out[[delta_iter]]$g_est$g_pred_natural_A1
    g_pred_A0 <- g_out[[delta_iter]]$g_est$g_pred_natural_A0
    Da_numerator <- delta[delta_iter] * phi_est * (valid_data$A - g_pred_A1)
    Da_denominator <- (delta[delta_iter] * g_pred_A1 + g_pred_A0)^2
    D_A <- Da_numerator / Da_denominator

    # compute component Dy from nuisance parameters
    m_pred_obs <- valid_data$A * m_out$m_est$m_pred_Ais1 +
      (1 - valid_data$A) * m_out$m_est$m_pred_Ais0

    # stabilize weights (dividing by sample average) and compute IPW estimate
    g_shifted <- valid_data$A * g_out[[delta_iter]]$g_est$g_pred_shifted_A1 +
      (1 - valid_data$A) * g_out[[delta_iter]]$g_est$g_pred_shifted_A0

    e_pred <- valid_data$A * e_out$e_est$e_pred_natural_A1 +
      (1 - valid_data$A) * e_out$e_est$e_pred_natural_A0
    mean_weights <- mean(g_shifted / e_pred)

    # compute component of EIF associated to Y
    D_Y <- ((g_shifted / e_pred) / mean_weights) * (valid_data$Y - m_pred_obs)

    # output table of EIF results for given shift
    eif_out <- data.table::data.table(
      Dy = D_Y, Da = D_A, Dzw = D_ZW,
      fold = origami::fold_index()
    )
    return(eif_out)
  })

  # output
  return(eif_over_delta)
}

###############################################################################

#' Construct joint mediator-baseline score of efficient influence function
#'
#' @param g_out Object containing results from fitting the propensity score
#'  regression, as produced by a call to \code{\link{fit_g_mech}}.
#' @param m_out Object containing results from fitting the outcome
#'  regression, as produced by a call to \code{\link{fit_m_mech}}.
#'
#' @keywords internal
compute_Dzw <- function(g_out, m_out) {
  # get g components from output for that nuisance parameter
  g_shifted_A1 <- g_out$g_est$g_pred_shifted_A1
  g_shifted_A0 <- g_out$g_est$g_pred_shifted_A0

  # get m components from output for that nuisance parameter
  m_pred_Ais1 <- m_out$m_est$m_pred_Ais1
  m_pred_Ais0 <- m_out$m_est$m_pred_Ais0

  # compute component Dzw from nuisance parameters
  Dzw_A1 <- g_shifted_A1 * m_pred_Ais1
  Dzw_A0 <- g_shifted_A0 * m_pred_Ais0

  # output as simple list
  return(list(
    dzw_cntrl = Dzw_A0,
    dzw_treat = Dzw_A1
  ))
}

###############################################################################

#' Cross-validated Efficient Influence Function for Interventional Effects
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medshift}}.
#' @param delta A \code{numeric} value indicating the magnitude of shift in the
#'  treatment that defines the counterfactual quantity of interest. In the case
#'  of binary treatments, this defines the incremental propensity score shift,
#'  which acts as a multiplier of the odds with which an observational unit
#'  receives treatment (EH Kennedy, 2018; <doi:10.1080/01621459.2017.1422737>).
#'  When the treatment is quantitative, the defines the degree of shift for a
#'  modified treatment policy but non-binary treatments are not  yet supported.
#' @param g_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model.
#' @param e_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a reparametrized model
#'  for the mediator(s) that is equivalent to a propensity score conditioning
#'  on the mediators.
#' @param b_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model
#'  for the intermediate confounder, adjusting for the baseline covariates and
#'  the treatment both. This is irrelevant for stochastic (in)direct effects,
#'  and so is ignored when L is left to its default of \code{NULL}.
#' @param d_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model
#'  for the intermediate confounder, adjusting for the baseline covariates, the
#'  treatment, and the mediators. This is irrelevant for stochastic (in)direct
#'  effects, and so is ignored when L is left to its default of \code{NULL}.
#' @param m_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit an outcome regression.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by a call to the wrapper function \code{medshift}.
#' @param z_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (Z). The input for this argument is automatically
#'  generated by a call to the wrapper function \code{medshift}.
#'
#' @importFrom data.table data.table as.data.table setnames rbindlist copy ":="
#' @importFrom origami cross_validate training validation make_folds
#'  folds_vfold
#' @importFrom sl3 sl3_Task Lrnr_hal9001
#' @importFrom stats sd
#'
#' @keywords internal
interv_cv_eif <- function(fold,
                          data,
                          delta,
                          g_learners,
                          e_learners,
                          b_learners,
                          d_learners,
                          m_learners,
                          w_names,
                          z_names) {
  # make training and validation data
  train_data <- origami::training(data)
  valid_data <- origami::validation(data)

  ## 1) fit propensity score w/ IPSI p(A | W) and p(A_delta | W)
  g_out <- fit_g_mech(
    data = train_data, valid_data = valid_data, delta = delta,
    learners = g_learners, w_names = w_names
  )

  ## 2) fit reparameterized propensity score cond. on mediators p(A | Z, W)
  e_out <- fit_e_mech(
    data = train_data, valid_data = valid_data, learners = e_learners,
    z_names = z_names, w_names = w_names
  )

  ## 3) fit intermediate confounding mech. w/o mediators p(L | A, W)
  b_out <- fit_b_mech(
    data = train_data, valid_data = valid_data, learners = b_learners,
    w_names = w_names
  )

  ## 4) fit intermediate confounding mech. cond. on mediators p(L | Z, A, W)
  d_out <- fit_d_mech(
    data = train_data, valid_data = valid_data, learners = d_learners,
    z_names = z_names, w_names = w_names
  )

  ## 5) fit standard outcome regression E[Y | Z, L, A, W]
  m_out <- fit_m_mech(
    data = train_data, valid_data = valid_data, learners = m_learners,
    z_names = z_names, w_names = w_names
  )

  ## compute "natural" estimates wrt observed A and L
  g_pred_Anat <- valid_data$A * g_out$g_est$g_pred_natural_A1 +
    (1 - valid_data$A) * g_out$g_est$g_pred_natural_A0
  g_pred_shifted_Anat <- valid_data$A * g_out$g_est$g_pred_shifted_A1 +
    (1 - valid_data$A) * g_out$g_est$g_pred_shifted_A0
  e_pred_Anat <- valid_data$A * e_out$e_est$e_pred_natural_A1 +
    (1 - valid_data$A) * e_out$e_est$e_pred_natural_A0
  b_pred_Lnat <- valid_data$L * b_out$b_est$b_pred_L1_Anat +
    (1 - valid_data$L) * b_out$b_est$b_pred_L0_Anat
  d_pred_Lnat <- valid_data$L * d_out$d_est$d_pred_L1_Anat +
    (1 - valid_data$L) * d_out$d_est$d_pred_L0_Anat

  ## similarly compute "natural" estimates for outcome regression but across
  ## "binary slices" (i.e., 2x2 table) of observed A and L
  m_pred_Anat_Lis1 <- valid_data$A * m_out$m_est$m_pred_Ais1_Lis1 +
    (1 - valid_data$A) * m_out$m_est$m_pred_Ais0_Lis1
  m_pred_Anat_Lis0 <- valid_data$A * m_out$m_est$m_pred_Ais1_Lis0 +
    (1 - valid_data$A) * m_out$m_est$m_pred_Ais0_Lis0
  m_pred_Anat_Lnat <- valid_data$L * m_pred_Anat_Lis1 +
    (1 - valid_data$L) * m_pred_Anat_Lis0

  ## compute "derived" nuisance estimates
  u_pred_Anat <- m_pred_Anat_Lis1 * b_out$b_est$b_pred_L1_Anat +
    m_pred_Anat_Lis0 * b_out$b_est$b_pred_L0_Anat
  u_pred_Ais1 <- m_out$m_est$m_pred_Ais1_Lis1 * b_out$b_est$b_pred_L1_Ais1 +
    m_out$m_est$m_pred_Ais1_Lis0 * (1 - b_out$b_est$b_pred_L1_Ais1)
  u_pred_Ais0 <- m_out$m_est$m_pred_Ais0_Lis1 * b_out$b_est$b_pred_L1_Ais0 +
    m_out$m_est$m_pred_Ais0_Lis0 * (1 - b_out$b_est$b_pred_L1_Ais0)
  v_pred <- m_pred_Anat_Lnat * b_out$b_est$b_pred_L1_Anat /
    d_out$d_est$d_pred_L1_Anat
  s_pred <- m_pred_Anat_Lnat * b_out$b_est$b_pred_L1_Anat /
    d_out$d_est$d_pred_L1_Anat * g_out$g_est$g_pred_natural_A1 /
    e_out$e_est$e_pred_natural_A1

  # NOTE: the logic below attempts to get validation predictions for several
  #       derived nuisances by assuming these to have been estimated on the
  #       training data, but this implementation doesn't keep such estimates.
  #       rather than implementing an algorithm w/ multiple cross-validation
  #       loops, we will instead use nested CV as begun below, though this is
  #       very likely less "stable" than the training-validation split reuse.

  # TODO: reimplement nuisance estimation below using nested CV instead of
  #       training-validation resplits
  n_folds <- (train_data[, .N] + valid_data[, .N]) / valid_data[, .N]
  nested_nuisance_folds <- origami::make_folds(
    valid_data,
    fold_fun = origami::folds_vfold,
    V = n_folds
  )
  validx <- lapply(nested_nuisance_folds, `[[`, "validation_set")
  reorder_validx <- order(do.call(c, validx))

  ## instantiate HAL and GLM for pseudo-outcome regressions
  hal_learner <- sl3::Lrnr_hal9001$new(
    max_degree = 3L,
    family = "gaussian",
    return_lasso = FALSE,
    yolo = FALSE
  )

  if (stats::sd(v_pred) < .Machine$double.eps * 10) {
    v_pred_Lis1 <- v_pred_Lis0 <- rep(mean(v_pred), length(v_pred))
  } else {
    # helper to estimate pseudo-outcome nuisance by nested cross-validation
    v_cfpred_cvfit <- function(fold,
                               v_data,
                               v_learners,
                               w_names) {
      # create nested training-validation splits
      v_data_train <- origami::training(v_data)
      v_data_valid <- origami::validation(v_data)

      # nested cross-validated task for training
      v_task_train <- sl3::sl3_Task$new(
        data = v_data_train[],
        covariates = c(w_names, "A", "L"),
        outcome = "v_est",
        outcome_type = "gaussian"
      )

      # fit pseudo-outcome regression
      v_fit <- v_learners$train(v_task_train)

      # tasks for counterfactual predictions
      v_task_valid_Lis1 <- sl3::sl3_Task$new(
        data = v_data_valid[, L := 1][],
        covariates = c(w_names, "A", "L"),
        outcome = "v_est",
        outcome_type = "gaussian"
      )
      v_task_valid_Lis0 <- sl3::sl3_Task$new(
        data = v_data_valid[, L := 0][],
        covariates = c(w_names, "A", "L"),
        outcome = "v_est",
        outcome_type = "gaussian"
      )

      # generate counterfactual predictions
      v_pred_Lis1 <- v_fit$predict(v_task_valid_Lis1)
      v_pred_Lis0 <- v_fit$predict(v_task_valid_Lis0)
      v_cfpred_out <- data.table::data.table(
        v_pred_Lis1 = v_pred_Lis1, v_pred_Lis0 = v_pred_Lis0
      )
      return(list(v_cfpred_out))
    }

    # make data for estimating v under contrasts of L
    v_data <- data.table::copy(valid_data)
    v_data[, v_est := v_pred]

    v_cfpred_cvfit_results <- origami::cross_validate(
      cv_fun = v_cfpred_cvfit,
      folds = nested_nuisance_folds,
      v_data = v_data,
      v_learners = hal_learner,
      w_names = w_names,
      use_future = FALSE,
      .combine = FALSE
    )
    v_cfpreds <- data.table::rbindlist(v_cfpred_cvfit_results[[1]])
    v_cfpreds <- v_cfpreds[reorder_validx, ]
    v_pred_Lis1 <- v_cfpreds$v_pred_Lis1
    v_pred_Lis0 <- v_cfpreds$v_pred_Lis0
  }


  if (stats::sd(s_pred) < .Machine$double.eps * 10) {
    s_pred_Lis1 <- s_pred_Lis0 <- rep(mean(s_pred), length(s_pred))
  } else {
    # helper to estimate pseudo-outcome nuisance by nested cross-validation
    s_cfpred_cvfit <- function(fold,
                               s_data,
                               s_learners,
                               w_names) {
      # create nested training-validation splits
      s_data_train <- origami::training(s_data)
      s_data_valid <- origami::validation(s_data)

      # nested cross-validated task for training
      s_task_train <- sl3::sl3_Task$new(
        data = s_data_train[],
        covariates = c(w_names, "A", "L"),
        outcome = "s_est",
        outcome_type = "gaussian"
      )

      # fit pseudo-outcome regression
      s_fit <- s_learners$train(s_task_train)

      # tasks for counterfactual predictions
      s_task_valid_Lis1 <- sl3::sl3_Task$new(
        data = s_data_valid[, L := 1][],
        covariates = c(w_names, "A", "L"),
        outcome = "s_est",
        outcome_type = "gaussian"
      )
      s_task_valid_Lis0 <- sl3::sl3_Task$new(
        data = s_data_valid[, L := 0][],
        covariates = c(w_names, "A", "L"),
        outcome = "s_est",
        outcome_type = "gaussian"
      )

      # generate counterfactual predictions
      s_pred_Lis1 <- s_fit$predict(s_task_valid_Lis1)
      s_pred_Lis0 <- s_fit$predict(s_task_valid_Lis0)
      s_cfpred_out <- data.table::data.table(
        s_pred_Lis1 = s_pred_Lis1, s_pred_Lis0 = s_pred_Lis0
      )
      return(list(s_cfpred_out))
    }

    # make data for estimating v under contrasts of L
    s_data <- data.table::copy(valid_data)
    s_data[, s_est := s_pred]

    s_cfpred_cvfit_results <- origami::cross_validate(
      cv_fun = s_cfpred_cvfit,
      folds = nested_nuisance_folds,
      s_data = s_data,
      s_learners = hal_learner,
      w_names = w_names,
      use_future = FALSE,
      .combine = FALSE
    )
    s_cfpreds <- data.table::rbindlist(s_cfpred_cvfit_results[[1]])
    s_cfpreds <- s_cfpreds[reorder_validx, ]
    s_pred_Lis1 <- s_cfpreds$s_pred_Lis1
    s_pred_Lis0 <- s_cfpreds$s_pred_Lis0
  }


  if (stats::sd(u_pred_Anat) < .Machine$double.eps * 10) {
    u_pred_Ais1_intZ <- u_pred_Ais0_intZ <-
      rep(mean(u_pred_Anat), length(u_pred_Anat))
  } else {
    # helper to estimate pseudo-outcome nuisance by nested cross-validation
    u_intZ_cfpred_cvfit <- function(fold,
                                    u_data,
                                    u_learners,
                                    w_names) {
      # create nested training-validation splits
      u_intZ_data_train <- origami::training(u_data)
      u_intZ_data_valid <- origami::validation(u_data)

      # nested cross-validated task for training
      u_intZ_task_train <- sl3::sl3_Task$new(
        data = u_intZ_data_train[],
        covariates = c(w_names, "A"),
        outcome = "u_est",
        outcome_type = "gaussian"
      )

      # fit pseudo-outcome regression
      u_intZ_fit <- u_learners$train(u_intZ_task_train)

      # tasks for counterfactual predictions
      u_intZ_task_valid_Ais1 <- sl3::sl3_Task$new(
        data = u_intZ_data_valid[, A := 1][],
        covariates = c(w_names, "A"),
        outcome = "u_est",
        outcome_type = "gaussian"
      )
      u_intZ_task_valid_Ais0 <- sl3::sl3_Task$new(
        data = u_intZ_data_valid[, A := 0][],
        covariates = c(w_names, "A"),
        outcome = "u_est",
        outcome_type = "gaussian"
      )

      # generate counterfactual predictions
      u_pred_Ais1_intZ <- u_intZ_fit$predict(u_intZ_task_valid_Ais1)
      u_pred_Ais0_intZ <- u_intZ_fit$predict(u_intZ_task_valid_Ais0)
      u_intZ_cfpred_out <- data.table::data.table(
        u_pred_Ais1_intZ = u_pred_Ais1_intZ,
        u_pred_Ais0_intZ = u_pred_Ais0_intZ
      )
      return(list(u_intZ_cfpred_out))
    }

    # make data for estimating v under contrasts of L
    u_data <- data.table::copy(valid_data)
    u_data[, u_est := u_pred_Anat]

    u_intZ_cfpred_cvfit_results <- origami::cross_validate(
      cv_fun = u_intZ_cfpred_cvfit,
      folds = nested_nuisance_folds,
      u_data = u_data,
      u_learners = hal_learner,
      w_names = w_names,
      use_future = FALSE,
      .combine = FALSE
    )
    u_intZ_cfpreds <- data.table::rbindlist(u_intZ_cfpred_cvfit_results[[1]])
    u_intZ_cfpreds <- u_intZ_cfpreds[reorder_validx, ]
    u_pred_Ais1_intZ <- u_intZ_cfpreds$u_pred_Ais1_intZ
    u_pred_Ais0_intZ <- u_intZ_cfpreds$u_pred_Ais0_intZ
  }

  # NOTE: the next bit is only relevant to IPSI's
  u_pred_Anat_intZ <- valid_data$A * u_pred_Ais1_intZ +
    (1 - valid_data$A) * u_pred_Ais0_intZ
  u_diff_pred_intZ <- u_pred_Ais1_intZ - u_pred_Ais0_intZ

  if (stats::sd(u_pred_Ais1 - u_pred_Ais0) < .Machine$double.eps * 10) {
    u_diff_pred <- rep(mean(u_pred_Ais1 - u_pred_Ais0), length(u_pred_Ais1))
  } else {
    # helper to estimate pseudo-outcome nuisance by nested cross-validation
    u_diff_pred_cvfit <- function(fold,
                                  u_data,
                                  u_learners,
                                  w_names) {
      # create nested training-validation splits
      u_diff_data_train <- origami::training(u_data)
      u_diff_data_valid <- origami::validation(u_data)

      # nested cross-validated task for training
      u_diff_task_train <- sl3::sl3_Task$new(
        data = u_diff_data_train[],
        covariates = c(w_names),
        outcome = "u_diff_est",
        outcome_type = "gaussian"
      )

      # fit pseudo-outcome regression
      u_diff_fit <- u_learners$train(u_diff_task_train)

      # tasks for validation predictions
      u_diff_task_valid <- sl3::sl3_Task$new(
        data = u_diff_data_valid[],
        covariates = c(w_names),
        outcome = "u_diff_est",
        outcome_type = "gaussian"
      )

      # generate validation predictions and return
      u_diff_pred <- u_diff_fit$predict(u_diff_task_valid)
      return(list(u_diff_pred = u_diff_pred))
    }

    # make data for estimating v under contrasts of L
    u_diff_data <- data.table::copy(valid_data)
    u_diff_data[, u_diff_est := u_pred_Ais1 - u_pred_Ais0]

    u_diff_pred_cvfit_results <- origami::cross_validate(
      cv_fun = u_diff_pred_cvfit,
      folds = nested_nuisance_folds,
      u_data = u_diff_data,
      u_learners = hal_learner,
      w_names = w_names,
      use_future = FALSE,
      .combine = FALSE
    )
    u_diff_pred <- do.call(c, u_diff_pred_cvfit_results[[1]])
    u_diff_pred <- u_diff_pred[reorder_validx]
  }

  # return nuisance estimates for estimator construction
  cveif_est_out <- data.table::data.table(
    # NOTE: estimates of standard/direct nuisance parameters are below
    g_pred_Anat = g_pred_Anat,
    g_pred_shifted_Anat = g_pred_shifted_Anat,
    g_pred_shifted_A1 = g_out$g_est$g_pred_shifted_A1,
    g_pred_shifted_A0 = g_out$g_est$g_pred_shifted_A0,
    g_pred_natural_A1 = g_out$g_est$g_pred_natural_A1,
    g_pred_natural_A0 = g_out$g_est$g_pred_natural_A0,
    e_pred_Anat = e_pred_Anat,
    e_pred_natural_A1 = e_out$e_est$e_pred_natural_A1,
    e_pred_natural_A0 = e_out$e_est$e_pred_natural_A0,
    b_pred_Lnat = b_pred_Lnat,
    b_pred_L1_Anat = b_out$b_est$b_pred_L1_Anat,
    b_pred_L1_Ais1 = b_out$b_est$b_pred_L1_Ais1,
    b_pred_L1_Ais0 = b_out$b_est$b_pred_L1_Ais0,
    d_pred_Lnat = d_pred_Lnat,
    d_pred_L1_Ais1 = d_out$d_est$d_pred_L1_Ais1,
    d_pred_L1_Ais0 = d_out$d_est$d_pred_L1_Ais0,
    m_pred_Lnat_Anat = m_pred_Anat_Lnat,
    m_pred_Lis1_Ais1 = m_out$m_est$m_pred_Ais1_Lis1,
    m_pred_Lis1_Ais0 = m_out$m_est$m_pred_Ais0_Lis1,
    m_pred_Lis0_Ais1 = m_out$m_est$m_pred_Ais1_Lis0,
    m_pred_Lis0_Ais0 = m_out$m_est$m_pred_Ais0_Lis0,
    # NOTE: estimates of "derived" nuisance parameters are below
    v_pred_Lis1 = v_pred_Lis1,
    v_pred_Lis0 = v_pred_Lis0,
    s_pred_Lis1 = s_pred_Lis1,
    s_pred_Lis0 = s_pred_Lis0,
    u_pred_Ais1 = u_pred_Ais1,
    u_pred_Ais1_intZ = u_pred_Ais1_intZ,
    u_pred_Ais0 = u_pred_Ais0,
    u_pred_Ais0_intZ = u_pred_Ais0_intZ,
    u_pred_Anat = u_pred_Anat,
    u_pred_Anat_intZ = u_pred_Anat_intZ,
    u_diff_pred = u_diff_pred,
    u_diff_pred_intZ = u_diff_pred_intZ
  )
  return(list(cveif_est_out))
}
