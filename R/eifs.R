utils::globalVariables(c("..eif_component_names", "..w_names"))

#' Cross-validated Efficient Influence Function for Stochastic Effects
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medshift}}.
#' @param delta A \code{numeric} value indicating the magnitude of shift in the
#'  treatment that defines the counterfactual quantity of interest. In the case
#'  of binary treatments, this defines the incremental propensity score shift,
#'  which acts as a multiplier of the odds with which an observational unit
#'  receives treatment (EH Kennedy, 2018; <doi:10.1080/01621459.2017.1422737>).
#'  When the treatment is quantitative, the defines the degree of shift for a
#'  modified treatment policy but non-binary treatments are not  yet supported.
#' @param g_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model.
#' @param e_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a reparametrized model
#'  for the mediator(s) that is equivalent to a propensity score conditioning
#'  on the mediators.
#' @param m_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit an outcome regression.
#' @param phi_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a nuisance regression of
#'  a "derived" pseudo-outcome (contrast of outcome regressions under different
#'  treatment conditions), conditional on baseline covariates. This is of the
#'  following form phi(W) = E[m(A = 1, Z, W) - m(A = 0, Z, W) | W).
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by a call to the wrapper function \code{medshift}.
#' @param z_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (Z). The input for this argument is automatically
#'  generated by a call to the wrapper function \code{medshift}.
#'
#' @importFrom data.table data.table
#' @importFrom origami training validation fold_index
#'
#' @keywords internal
stoch_cv_eif <- function(fold,
                         data,
                         delta,
                         g_learners,
                         e_learners,
                         m_learners,
                         phi_learners,
                         w_names,
                         z_names) {
  # make training and validation data
  train_data <- origami::training(data)
  valid_data <- origami::validation(data)

  # compute nuisance parameters eta = (g, m, e, phi)
  ## 1) fit regression for incremental propensity score intervention
  g_out <- lapply(delta, function(delta) {
    # NOTE: even this is _repeated_ computation since delta is a multiplier
    #       ...worth fixing later if a bottleneck.
    g_est_delta <- fit_g_mech(
      data = train_data, valid_data = valid_data,
      delta = delta,
      learners = g_learners, w_names = w_names
    )
    return(g_est_delta)
  })

  ## 2) fit clever regression for treatment, conditional on mediators
  e_out <- fit_e_mech(
    data = train_data, valid_data = valid_data,
    learners = e_learners,
    z_names = z_names, w_names = w_names
  )

  ## 3) fit regression for incremental propensity score intervention
  m_out <- fit_m_mech(
    data = train_data, valid_data = valid_data,
    learners = m_learners,
    z_names = z_names, w_names = w_names
  )

  ## 4) difference-reduced dimension regression with pseudo-outcome
  phi_est <- fit_phi_mech(
    train_data = train_data, valid_data = valid_data,
    learners = phi_learners,
    m_out = m_out, w_names = w_names
  )

  # loop over each delta-shift in grid to assemble EIF components
  eif_over_delta <- lapply(seq_along(delta), function(delta_iter) {
    # compute component Dzw from nuisance parameters
    Dzw_groupwise <- compute_Dzw(
      g_out = g_out[[delta_iter]], m_out = m_out
    )
    D_ZW <- Dzw_groupwise$dzw_cntrl + Dzw_groupwise$dzw_treat

    # compute component Da from nuisance parameters
    g_pred_A1 <- g_out[[delta_iter]]$g_est$g_pred_natural_A1
    g_pred_A0 <- g_out[[delta_iter]]$g_est$g_pred_natural_A0
    Da_numerator <- delta[delta_iter] * phi_est * (valid_data$A - g_pred_A1)
    Da_denominator <- (delta[delta_iter] * g_pred_A1 + g_pred_A0)^2
    D_A <- Da_numerator / Da_denominator

    # compute component Dy from nuisance parameters
    m_pred_obs <- valid_data$A * m_out$m_est$m_pred_Ais1 +
      (1 - valid_data$A) * m_out$m_est$m_pred_Ais0

    # stabilize weights (dividing by sample average) and compute IPW estimate
    g_shifted <- valid_data$A * g_out[[delta_iter]]$g_est$g_pred_shifted_A1 +
      (1 - valid_data$A) * g_out[[delta_iter]]$g_est$g_pred_shifted_A0

    e_pred <- valid_data$A * e_out$e_est$e_pred_natural_A1 +
      (1 - valid_data$A) * e_out$e_est$e_pred_natural_A0
    mean_weights <- mean(g_shifted / e_pred)

    # compute component of EIF associated to Y
    D_Y <- ((g_shifted / e_pred) / mean_weights) * (valid_data$Y - m_pred_obs)

    # output table of EIF results for given shift
    eif_out <- data.table::data.table(
      Dy = D_Y, Da = D_A, Dzw = D_ZW,
      fold = origami::fold_index()
    )
    return(eif_out)
  })

  # output
  return(eif_over_delta)
}

###############################################################################

#' Construct joint mediator-baseline score of efficient influence function
#'
#' @param g_out Object containing results from fitting the propensity score
#'  regression, as produced by a call to \code{\link{fit_g_mech}}.
#' @param m_out Object containing results from fitting the outcome
#'  regression, as produced by a call to \code{\link{fit_m_mech}}.
#'
#' @keywords internal
compute_Dzw <- function(g_out, m_out) {
  # get g components from output for that nuisance parameter
  g_shifted_A1 <- g_out$g_est$g_pred_shifted_A1
  g_shifted_A0 <- g_out$g_est$g_pred_shifted_A0

  # get m components from output for that nuisance parameter
  m_pred_Ais1 <- m_out$m_est$m_pred_Ais1
  m_pred_Ais0 <- m_out$m_est$m_pred_Ais0

  # compute component Dzw from nuisance parameters
  Dzw_A1 <- g_shifted_A1 * m_pred_Ais1
  Dzw_A0 <- g_shifted_A0 * m_pred_Ais0

  # output as simple list
  return(list(
    dzw_cntrl = Dzw_A0,
    dzw_treat = Dzw_A1
  ))
}

###############################################################################

#' Cross-validated Efficient Influence Function for Interventional Effects
#'
#' @param fold Object specifying cross-validation folds as generated by a call
#'  to \code{origami::make_folds}.
#' @param data A \code{data.table} containing the observed data, with columns
#'  in the order specified by the NPSEM (Y, Z, A, W), with column names set
#'  appropriately based on the original input data. Such a structure is merely
#'  a convenience utility to passing data around to the various core estimation
#'  routines and is automatically generated by \code{\link{medshift}}.
#' @param delta A \code{numeric} value indicating the magnitude of shift in the
#'  treatment that defines the counterfactual quantity of interest. In the case
#'  of binary treatments, this defines the incremental propensity score shift,
#'  which acts as a multiplier of the odds with which an observational unit
#'  receives treatment (EH Kennedy, 2018; <doi:10.1080/01621459.2017.1422737>).
#'  When the treatment is quantitative, the defines the degree of shift for a
#'  modified treatment policy but non-binary treatments are not  yet supported.
#' @param g_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model.
#' @param e_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a reparametrized model
#'  for the mediator(s) that is equivalent to a propensity score conditioning
#'  on the mediators.
#' @param b_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model
#'  for the intermediate confounder, adjusting for the baseline covariates and
#'  the treatment both. This is irrelevant for stochastic (in)direct effects,
#'  and so is ignored when L is left to its default of \code{NULL}.
#' @param d_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit a propensity score model
#'  for the intermediate confounder, adjusting for the baseline covariates, the
#'  treatment, and the mediators. This is irrelevant for stochastic (in)direct
#'  effects, and so is ignored when L is left to its default of \code{NULL}.
#' @param m_learners A \code{\link[sl3]{Stack}} (or other learner class that
#'  inherits from \code{\link[sl3]{Lrnr_base}}), containing a single or set of
#'  instantiated learners from \pkg{sl3}, used to fit an outcome regression.
#' @param w_names A \code{character} vector of the names of the columns that
#'  correspond to baseline covariates (W). The input for this argument is
#'  automatically generated by a call to the wrapper function \code{medshift}.
#' @param z_names A \code{character} vector of the names of the columns that
#'  correspond to mediators (Z). The input for this argument is automatically
#'  generated by a call to the wrapper function \code{medshift}.
#'
#' @importFrom data.table data.table
#' @importFrom origami training validation fold_index
#'
#' @keywords internal
interv_cv_eif <- function(fold,
                          data,
                          delta,
                          g_learners,
                          e_learners,
                          b_learners,
                          d_learners,
                          m_learners,
                          w_names,
                          z_names) {
  # make training and validation data
  train_data <- origami::training(data)
  valid_data <- origami::validation(data)

  ## 1) fit propensity score w/ IPSI p(A | W) and p(A_delta | W)
  g_out <- fit_g_mech(
    data = train_data, valid_data = valid_data, delta = delta,
    learners = g_learners, w_names = w_names
  )

  ## 2) fit reparameterized propensity score cond. on mediators p(A | Z, W)
  e_out <- fit_e_mech(
    data = train_data, valid_data = valid_data, learners = e_learners,
    z_names = z_names, w_names = w_names
  )

  ## 3) fit intermediate confounding mech. w/o mediators p(L | A, W)
  b_out <- fit_b_mech(
    data = train_data, valid_data = valid_data, learners = b_learners,
    w_names = w_names
  )

  ## 4) fit intermediate confounding mech. cond. on mediators p(L | Z, A, W)
  d_out <- fit_d_mech(
    data = train_data, valid_data = valid_data, learners = d_learners,
    z_names = z_names, w_names = w_names
  )

  ## 5) fit standard outcome regression E[Y | Z, L, A, W]
  m_out <- fit_m_mech(
    data = train_data, valid_data = valid_data, learners = m_learners,
    z_names = z_names, w_names = w_names
  )

  ## compute "natural" estimates wrt observed A and L
  g_pred_Anat <- valid_data$A * g_out$g_est$g_pred_natural_A1 +
    (1 - valid_data$A) * g_out$g_est$g_pred_natural_A0
  e_pred_Anat <- valid_data$A * e_out$e_est$e_pred_natural_A1 +
    (1 - valid_data$A) * e_out$e_est$e_pred_natural_A0
  b_pred_Lnat <- valid_data$L * b_out$b_est$b_pred_L1_Anat +
    (1 - valid_data$L) * b_out$b_est$b_pred_L0_Anat
  d_pred_Lnat <- valid_data$L * d_out$d_pred_L1_Anat +
    (1 - valid_data$L) * d_out$d_pred_L0_Anat

  ## similarly compute "natural" estimates for outcome regression but across
  ## "binary slices" (i.e., 2x2 table) of observed A and L
  m_pred_Anat_Lis1 <- valid_data$A * m_out$m_est$m_pred_Ais1_Lis1 +
    (1 - valid_data$A) * m_out$m_est$m_pred_Ais0_Lis1
  m_pred_Anat_Lis0 <- valid_data$A * m_out$m_est$m_pred_Ais1_Lis0 +
    (1 - valid_data$A) * m_out$m_est$m_pred_Ais0_Lis0
  m_pred_Anat_Lnat <- valid_data$L * m_pred_Anat_Lis1 +
    (1 - valid_data$L) * m_pred_Anat_Lis0

  ## compute "derived" nuisance estimates
  u_pred_Anat <- m_pred_Anat_Lis1 * b_out$b_est$b_pred_L1_Anat +
    m_pred_Anat_Lis0 * b_out$b_est$b_pred_L0_Anat
  u_pred_Ais1 <- m_out$m_est$m_pred_Ais1_Lis1 * b_out$b_est$b_pred_L1_Ais1 +
    m_out$m_est$m_pred_Ais1_Lis0 * (1 - b_out$b_est$b_pred_L1_Ais1)
  u_pred_Ais0 <- m_out$m_est$m_pred_Ais0_Lis1 * b_out$b_est$b_pred_L1_Ais0 +
    m_out$m_est$m_pred_Ais0_Lis0 * (1 - b_out$b_est$b_pred_L1_Ais0)
  v_pred <- m_pred_Anat_Lnat * b_out$b_est$b_pred_L1_Anat /
    d_out$d_est$d_pred_L1_Anat
  s_pred <- m_pred_Anat_Lnat * b_out$b_est$b_pred_L1_Anat /
    d_out$d_est$d_pred_L1_Anat * g_out$g_est$g_pred_natural_A1 /
    e_out$e_est$e_pred_natural_A1

  # NOTE: the logic below attempts to get validation predictions for several
  #       derived nuisances by assuming these to have been estimated on the
  #       training data, but this implementation doesn't keep such estimates.
  #       rather than implementing an algorithm w/ multiple cross-validation
  #       loops, we will instead use nested CV as begun below, though this is
  #       very likely less "stable" than the training-validation split reuse.
  browser()

  # TODO: reimplement nuisance estimation below using nested CV instead of
  #       training-validation resplits
  nested_resplit_folds <- origami::make_folds(
    valid_data,
    fold_fun = origami::folds_vfold,
    V = 2L
  )

  if (stats::sd(v_pred) < .Machine$double.eps * 10) {
    v_pred_Lis1 <- v_pred_Lis0 <- rep(mean(v_pred), length(v_pred))
  } else {
    # make data for estimating v under contrasts of L
    v_data <- data.table::copy(valid_data)
    v_data[, v_est := v_pred]


    # cross-validated counterfactual predictions
    v_cv_cf_preds <- lapply(seq_along(val_idx), function(fold_idx) {
      v_task <- sl3::sl3_Task$new(
        data = v_data[trn_idx[[fold_idx]], ],
        covariates = c(names_w, "A", "L"),
        outcome = "vout",
        outcome_type = "gaussian"
      )

      # safely fit pseudo-outcome regression
      v_fit <- hal_learner$train(v_task)

      # counterfactual tasks
      v_task_L1 <- sl3_Task$new(
        data = v_data[val_idx[[fold_idx]], ][, L := 1][],
        covariates = c(names_w, "A", "L"),
        outcome = "vout",
        outcome_type = "gaussian"
      )
      v_task_L0 <- sl3_Task$new(
        data = v_data[val_idx[[fold_idx]], ][, L := 0][],
        covariates = c(names_w, "A", "L"),
        outcome = "vout",
        outcome_type = "gaussian"
      )

      # generate counterfactual predictions
      v1 <- v_fit$predict(v_task_L1)
      v0 <- v_fit$predict(v_task_L0)
      out <- data.table::as.data.table(list(v1, v0))
      data.table::setnames(out, c("v1", "v0"))
      return(out)
    })
    v_cv_cf_preds <- data.table::rbindlist(v_cv_cf_preds)
    v_cv_cf_preds <- v_cv_cf_preds[reorder_by_val_idx, ]
    v1 <- v_cv_cf_preds$v1
    v0 <- v_cv_cf_preds$v0
  }

  if (sd(sout) < .Machine$double.eps * 10) {
    s1 <- s0 <- rep(mean(sout), length(sout))
  } else {
    # make data for estimating s
    s_data <- data.table::as.data.table(list(W, A, L, sout))
    data.table::setnames(s_data, c(names_w, "A", "L", "sout"))

    # cross-validated counterfactual predictions
    s_cv_cf_preds <- lapply(seq_along(val_idx), function(fold_idx) {
      s_task <- sl3_Task$new(
        data = s_data[trn_idx[[fold_idx]], ],
        covariates = c(names_w, "A", "L"),
        outcome = "sout",
        outcome_type = "gaussian"
      )

      # safely fit pseudo-outcome regression
      s_fit <- hal_learner$train(s_task)

      # counterfactual tasks
      s_task_L1 <- sl3_Task$new(
        data = s_data[val_idx[[fold_idx]], ][, L := 1][],
        covariates = c(names_w, "A", "L"),
        outcome = "sout",
        outcome_type = "gaussian"
      )
      s_task_L0 <- sl3_Task$new(
        data = s_data[val_idx[[fold_idx]], ][, L := 0][],
        covariates = c(names_w, "A", "L"),
        outcome = "sout",
        outcome_type = "gaussian"
      )

      # generate counterfactual predictions
      s1 <- s_fit$predict(s_task_L1)
      s0 <- s_fit$predict(s_task_L0)
      out <- data.table::as.data.table(list(s1, s0))
      data.table::setnames(out, c("s1", "s0"))
      return(out)
    })
    s_cv_cf_preds <- data.table::rbindlist(s_cv_cf_preds)
    s_cv_cf_preds <- s_cv_cf_preds[reorder_by_val_idx, ]
    s1 <- s_cv_cf_preds$s1
    s0 <- s_cv_cf_preds$s0
  }

  if (sd(uA) < .Machine$double.eps * 10) {
    ubar1 <- ubar0 <- rep(mean(uA), length(uA))
  } else {
    # make data for estimating ubar
    ubar_data <- data.table::as.data.table(list(W, A, uA))
    data.table::setnames(ubar_data, c(names_w, "A", "uA"))

    # cross-validated counterfactual predictions
    ubar_cv_cf_preds <- lapply(seq_along(val_idx), function(fold_idx) {
      ubar_task <- sl3_Task$new(
        data = ubar_data[trn_idx[[fold_idx]], ],
        covariates = c(names_w, "A"),
        outcome = "uA",
        outcome_type = "gaussian"
      )

      # safely fit pseudo-outcome regression
      ubar_fit <- hal_learner$train(ubar_task)

      # counterfactual tasks
      ubar_task_A1 <- sl3_Task$new(
        data = ubar_data[val_idx[[fold_idx]], ][, A := 1][],
        covariates = c(names_w, "A"),
        outcome = "uA",
        outcome_type = "gaussian"
      )
      ubar_task_A0 <- sl3_Task$new(
        data = ubar_data[val_idx[[fold_idx]], ][, A := 0][],
        covariates = c(names_w, "A"),
        outcome = "uA",
        outcome_type = "gaussian"
      )

      # generate counterfactual predictions
      ubar1 <- ubar_fit$predict(ubar_task_A1)
      ubar0 <- ubar_fit$predict(ubar_task_A0)
      out <- data.table::as.data.table(list(ubar1, ubar0))
      data.table::setnames(out, c("ubar1", "ubar0"))
      return(out)
    })
    ubar_cv_cf_preds <- data.table::rbindlist(ubar_cv_cf_preds)
    ubar_cv_cf_preds <- ubar_cv_cf_preds[reorder_by_val_idx, ]
    ubar1 <- ubar_cv_cf_preds$ubar1
    ubar0 <- ubar_cv_cf_preds$ubar0
  }

  # NOTE: the next bit is only relevant to IPSI's
  ubarA <- A * ubar1 + (1 - A) * ubar0
  q1 <- ubar1 - ubar0

  if (sd(u1 - u0) < .Machine$double.eps * 10) {
    q2 <- rep(mean(u1 - u0), length(u1))
  } else {
    # make data for estimating q2
    q2_data <- data.table::as.data.table(list(W, u1 - u0))
    data.table::setnames(q2_data, c(names_w, "u_diff"))

    # cross-validated predictions
    q2_cv_preds <- lapply(seq_along(val_idx), function(fold_idx) {
      q2_train_task <- sl3_Task$new(
        data = q2_data[trn_idx[[fold_idx]], ],
        covariates = names_w,
        outcome = "u_diff",
        outcome_type = "gaussian"
      )
      q2_valid_task <- sl3_Task$new(
        data = q2_data[val_idx[[fold_idx]], ],
        covariates = names_w,
        outcome = "u_diff",
        outcome_type = "gaussian"
      )

      # safely fit pseudo-outcome regression
      q2_fit <- hal_learner$train(q2_train_task)
      q2 <- q2_fit$predict(q2_valid_task)
    })
    q2 <- do.call(c, q2_cv_preds)[reorder_by_val_idx]
  }

  # compute IPSI shifts
  g1delta <- ipsi_delta(g1, delta)
  gdelta <- A * g1delta + (1 - A) * (1 - g1delta)

  # compute clever covariates
  HD <- b / d * (1 - gdelta / e)
  HI <- b / d * gdelta * (1 / e - 1 / g)
  KD <- v1 - v0 - gdelta / g * (s1 - s0)
  KI <- gdelta / g * (s1 - s0 - v1 + v0)
  MD <- -g1delta * (1 - g1delta) / (g1 * (1 - g1)) * q2
  MI <- g1delta * (1 - g1delta) / (g1 * (1 - g1)) * (q2 - q1)
  J <- -gdelta / g


  # compute (in)direct effects and EIF
  de <- ubar1 * g1 + ubar0 * (1 - g1) - (u1 * g1delta + u0 * (1 - g1delta))
  ie <- (u1 - ubar1) * g1delta + (u0 - ubar0) * (1 - g1delta)
  eifD <- HD * (Y - m) + KD * (L - b1A) + MD * (A - g1) +
    (uA - ubarA) + de
  eifI <- HI * (Y - m) + KI * (L - b1A) + MI * (A - g1) + J *
    (uA - ubarA) + ie


}
